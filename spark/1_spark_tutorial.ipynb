{"cells":[{"cell_type":"markdown","source":["# RDDs, Dataframes, and Datasets\n\n## RDDs\n\nResilient Distributed Datasets (We talked about these!). A new range of API's has been introduced to let people take advantage of Spark's parallel execution framework and fault tolerance without making the same set of mistakes.\n\n## Dataframes\n\n- RDD's with named columns.\n- Columnar storage\n  - Similar optimizations for OLAP queries as vertica\n- Memory Management (Tungsten)\n  - direct control of data storage in memory\n    - cpu cache, and read ahead\n  - largest source of performance increase\n- avoids java serialization (or other not as slow but still slow serialization)\n  - Kryo serialization\n  - compression\n- no garbage collection overhead\n- Execution plans (Catalyst Optimizer)\n  - rule based instead of cost-based optimizer\n\n## Datasets\n\nadds to Dataframes\n- compile time safety\n- API only available through the scala (python has no type safety)\n\nEncoders act as liason between JVM object and off-heap memory (the new formats introduced with Tungsten)"],"metadata":{}},{"cell_type":"markdown","source":["## Let's load a file\n\n1. select 'Tables'\n2. in new tab, select 'Create Table'\n3. we could really select anything (from file upload, s3, DBFS, or JDBC) here but for now we will upload the 'mallard.csv' from the vertica demo\n  (https://s3-us-west-2.amazonaws.com/cse599c-sp17/mallard.csv)\n4. select preview table\n5. we can name the table, select our file delimiter, etc.\n6. retrieve the DBFS path befor\n7. select 'create table'"],"metadata":{}},{"cell_type":"markdown","source":["# RDD's and an Introduction to the DataFram API\nIn this next section we will look at importing data from a csv file into an RDD and how to do basic queries with RDDs.\nWe will then look at how to convert an RDD into a Dataframe and repeate the same query with the Datafame API."],"metadata":{"collapsed":true}},{"cell_type":"code","source":["# set file path for mallard.csv import\n#mallardFilePath = '/FileStore/tables/<uuid>/mallard.csv'\n#mallardFilePath = ''\n\n# structure of mallard.csv\n# event-id,timestamp,location-long,location-lat"],"metadata":{"collapsed":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Import raw rdd convert to dataframe\nfrom pyspark.sql import types\n\n# Create RDD from csv file\n# pysqark RDD documentation: http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD\nmallard_rdd = sc.textFile(mallardFilePath)\n\n# .take(n) is the standard method to return the first n elements of the RDD\nmallard_rdd.take(10)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# display(rdd) does not work so we will make this small helper function to display an rdd\ndef rdd_head(rdd,n=3):\n  displayHTML(\"Count: {} <br> {}\".format(rdd.count(), \"<br>\".join(str(row) for row in rdd.take(n))) )\n\n# We notice two things: 1) the header row is still included and 2) each row is a string not split into columns.\n# sc.textFile simply makes a new row for each new line of the file and does not know it is a CSV.\nrdd_head(mallard_rdd)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# We want to skip the first row using filter()\n\n# Save header row for later \nheader_row = mallard_rdd.first()\n\n# Use a lambda function to create a new rdd without the header row.\nmallard_rdd = mallard_rdd.filter(lambda row : not row.startswith('event-id'))\n\n# The header row is now gone\nrdd_head(mallard_rdd)\n"],"metadata":{"collapsed":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":["#Split rows and convert\nfrom datetime import datetime\n\n\ndef make_row(row):\n  \"\"\"\n  Convert a csv row from mallard into a tuple data type\n  Input: row<str>\n  Output: tuple<int,datetime,float,float>\n  \"\"\"\n  row = row.split(',')\n  return ( int(row[0]), datetime.strptime(row[1],'%Y-%m-%d %H:%M:%S.000') , float(row[2]) , float(row[3]) )\n\nmallard_rdd = mallard_rdd.map(make_row)\n\n# Each row of mallard_rdd is now a tuple with the data split\nrdd_head(mallard_rdd)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["#Question: What day had the most sightings?"],"metadata":{}},{"cell_type":"code","source":["# Group the data by the date\nmallard_rdd_days = mallard_rdd.groupBy( \n  lambda row: str(row[1].date()) # return the date_str as the group by key\n) \\ #RDD with tuple rows (date_str, list of tuples from that date)\n.map(\n  lambda row: (row[0],len(row[1])) # tuple (date_str, len(sightings for that day) ) \n) \n\nrdd_head(mallard_rdd_days)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# Get the max dumper of sightings\nmallard_rdd_days.max( lambda row: row[1] )"],"metadata":{"collapsed":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# pyspark Dataframe documentation: http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n# For structured data the DataFrame API is much more powerful than the RDD API\nmallard_dataframe = mallard_rdd.toDF()\nmallard_dataframe.printSchema()  # Schema does't have column names (we'll fix this)\nmallard_dataframe.count()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Add column names to schema using header_row from above \nheader_row = header_row.split(',')\ndef row_func(row):\n  return { header : obj for header , obj in zip(header_row,row) }\n\nmallard_dataframe = mallard_rdd.map(row_func).toDF()\nmallard_dataframe.printSchema()\nmallard_dataframe.head(10)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# If the data is structured there is no need to go through an RDD first.  Spark now has a direct to DataFrame function.\nmallard = sqlContext.read.format(\"com.databricks.spark.csv\").options(header='true', inferschema='true', delimiter=',').load(mallardFilePath)\nmallard.printSchema()\ndisplay(mallard)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["#Question: What day had the most sightings?"],"metadata":{}},{"cell_type":"code","source":["#pyspark sql functions: http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#module-pyspark.sql.functions\n# offers programatic SQL type commands\nimport pyspark.sql.functions as sf\n\n# Answer the same question from above using the DataFrame\nmallard_days = mallard \\\n  .select(sf.col('timestamp').cast('date').alias('date')) \\\n  .groupBy('date') \\\n  .agg(sf.count(\"date\").alias(\"count\")) \\\n  .sort(sf.col(\"count\").desc())\n\ndisplay(mallard_days)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# A more complicated example to show the expressiveness of pysqark \n# Calculate number of sightings in 1.1km squares (~= 1 decimal places of latitude and longitude )\n\ncountOneKM = mallard.withColumn(\"round-long\",sf.round(\"location-long\",1)) \\\n  .withColumn(\"round-lat\",sf.round(\"location-lat\",1))\\\n  .select(\"round-long\",\"round-lat\") \\\n  .groupBy(\"round-long\", \"round-lat\") \\\n  .agg(sf.count(\"round-long\").alias('location-bin')) \\\n  .sort(sf.col(\"location-bin\").desc())\ndisplay(countOneKM)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["# Use Case: On-Time Flight Performance\n\nThis notebook provides an analysis of On-Time Flight Performance and Departure Delays\n\nSource Data: \n* [OpenFlights: Airport, airline and route data](http://openflights.org/data.html)\n* [United States Department of Transportation: Bureau of Transportation Statistics (TranStats)](http://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&DB_Short_Name=On-Time)\n * Note, the data used here was extracted from the US DOT:BTS between 1/1/2014 and 3/31/2014*\n\nReferences:\n* [GraphFrames User Guide](http://graphframes.github.io/user-guide.html)\n* [GraphFrames: DataFrame-based Graphs (GitHub)](https://github.com/graphframes/graphframes)\n* [D3 Airports Example](http://mbostock.github.io/d3/talk/20111116/airports.html)"],"metadata":{}},{"cell_type":"markdown","source":["### Preparation\nExtract the Airports and Departure Delays information from S3 / DBFS"],"metadata":{}},{"cell_type":"code","source":["# We may start by getting the DBFS paths. These are essentially just HDFS specifically for databricks\ntripdelaysFilePath = \"/databricks-datasets/flights/departuredelays.csv\"\nairportsnaFilePath = \"/databricks-datasets/flights/airport-codes-na.txt\""],"metadata":{"collapsed":true},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# Obtain airports data. The 'sqlContext.read.format' call is the standard method for structured data ingestion (in this case for a DataFrame)\nairportsna = sqlContext.read.format(\"com.databricks.spark.csv\").options(header='true', inferschema='true', delimiter='\\t').load(airportsnaFilePath)\nairportsna.registerTempTable(\"airports_na\")"],"metadata":{"collapsed":true},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# Obtain departure Delays data and also cache the results (materialize the 'intermediate' result in memory)\ndepartureDelays = sqlContext.read.format(\"com.databricks.spark.csv\").options(header='true').load(tripdelaysFilePath)\ndepartureDelays.registerTempTable(\"departureDelays\")\ndepartureDelays.cache()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":22},{"cell_type":"code","source":["# we can not print the schema for either DF\nairportsna.printSchema()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":23},{"cell_type":"code","source":["departureDelays.printSchema()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["### We can query our DF with both a programatic and SQL interface (via the registered table)\n\nit depends on which is more practical"],"metadata":{}},{"cell_type":"code","source":["# try and sort departueDelays on the delay column and preview the first 3 and save it as a new table sortDelays\nsortDelays = departureDelays.sort(\"delay\")\nsortDelays.head(3)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":26},{"cell_type":"code","source":["# We have already registered departure delays and we may write an equivalent query in SQL\n# use pysqark.sql.functions which is already imported as sf\ndepartureDelays.registerTempTable(\"depature_delays\")\nsortDelays_sql = sqlContext.sql(\"SELECT * FROM depature_delays ORDER BY delay\")\nsortDelays_sql.head(3)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":27},{"cell_type":"code","source":["# We might want to use group bys and aggregates. This is doable through the .agg and SQL function ('sf' from earlier)\n# find all desitinations whose average distance is greater than 1000\n# (note that queries for learning purposes need not make sense)\nlongAvgDistByDest = departureDelays.groupBy(\"destination\")\\\n  .agg(sf.avg(\"distance\").alias(\"avg_dist\"))\\\n  .where(\"avg_dist > 1000\")\nlongAvgDistByDest.head(3)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":28},{"cell_type":"code","source":["# That wasn't too tedious but it might be easier just to write it as a SQL command:\nlongAvgDistByDest_sql = sqlContext.sql(\"SELECT destination, avg(distance) AS avg_dist FROM depature_delays GROUP BY destination HAVING avg_dist > 1000\")\nlongAvgDistByDest_sql.head(3)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":29},{"cell_type":"code","source":["# we can also use the python fluent API to execute joins\n# lets get the average flight delay to every city from seattle\n# but we want the actual city name and not just the airport code\nfrom pyspark.sql.functions import col\ndelayedSeaDest = departureDelays.join(airportsna, departureDelays[\"destination\"] == airportsna[\"IATA\"], 'inner')\\\n  .filter(col(\"origin\") == 'SEA')\\\n  .groupBy(\"City\")\\\n  .agg(sf.avg(\"delay\").alias(\"avg_delay\"))\\\n  .orderBy(sf.col(\"avg_delay\").desc())\ndelayedSeaDest.head(3)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":30},{"cell_type":"code","source":["# this would likley be easier using the declarative API.\nairportsna.registerTempTable(\"airports\")\ndelayedSeaDest_sql = sqlContext.sql(\"SELECT destination, avg(delay) AS avg_delay FROM depature_delays, airports WHERE origin = 'SEA' AND destination = IATA GROUP BY destination ORDER BY -avg_delay\")\ndelayedSeaDest_sql.head(3)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":31},{"cell_type":"code","source":["# spark also allows you to look up the query plan.\n# We should observe that the plans look the same for both the SQL and programatic approaches \ndelayedSeaDest.explain()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":32},{"cell_type":"code","source":["delayedSeaDest_sql.explain()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":33},{"cell_type":"code","source":[""],"metadata":{"collapsed":true},"outputs":[],"execution_count":34}],"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython2","codemirror_mode":{"name":"ipython","version":2},"version":"2.7.13","nbconvert_exporter":"python","file_extension":".py"},"name":"Tutorial","notebookId":2056133347430777},"nbformat":4,"nbformat_minor":0}
