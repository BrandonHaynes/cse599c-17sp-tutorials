{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDDs, Dataframes, and Datasets\n",
    "\n",
    "note that underneath these are ALL run with RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframes\n",
    "\n",
    "RDD's with named *untyped* columns. Allows to add more layers of abstraction and automatic optimization\n",
    "\n",
    "2 important optimizations:\n",
    "- Memory Management (Tungsten)\n",
    "    - data is stored off-heat in binary files (less volume)\n",
    "    - avoids expensive java serialization (or other not as slow but still slow serialization)\n",
    "    - no garbage collection overhead\n",
    "- Execution plans (Catalyst Optimizer)\n",
    "\n",
    "typecheck errors only at runtime so..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "Let's make it strongly typed! We want compile time safety as well as OOP style data access for developers (similar to RDDs) but we also want the query optimizer from DFs.\n",
    "\n",
    "Encoders act as liason between JVM object and off-heap memory (the new formats introduced with Tungsten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Databricks provides this nice guide for when to use which API:\n",
    "\n",
    "- If you want rich semantics, high-level abstractions, and domain specific APIs, use DataFrame or Dataset.\n",
    "- If your processing demands high-level expressions, filters, maps, aggregation, averages, sum, SQL queries, columnar access and use of lambda functions on semi-structured data, use DataFrame or Dataset.\n",
    "- If you want higher degree of type-safety at compile time, want typed JVM objects, take advantage of Catalyst optimization, and benefit from Tungstenâ€™s efficient code generation, use Dataset.\n",
    "- If you want unification and simplification of APIs across Spark Libraries, use DataFrame or Dataset.\n",
    "- If you are a R user, use DataFrames.\n",
    "- If you are a Python user, use DataFrames and resort back to RDDs if you need more control."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
