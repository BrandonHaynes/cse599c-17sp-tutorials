{"cells":[{"cell_type":"markdown","source":["# RDDs, Dataframes, and Datasets\n\n## RDDs\n\nResilient Distributed Datasets (We talked about these!). A new range of API's has been introduced to let people take advantage of Spark's parallel execution framework and fault tolerance without making the same set of mistakes.\n\n## Dataframes\n\n- RDD's with named *untyped* columns.\n- Columnar storage\n  - Similar optimizations for OLAP queries as vertica\n- Memory Management (Tungsten)\n  - direct control of data storage in memory\n    - cpu cache, and read ahead\n  - largest source of performance increase\n- avoids java serialization (or other not as slow but still slow serialization)\n  - Kryo serialization\n  - compression\n- no garbage collection overhead\n- Execution plans (Catalyst Optimizer)\n  - rule based instead of cost-based optimizer\n\n## Datasets\n\nadds to Dataframes\n- compile time safety\n- API only available through the scala (python has no type safety)\n\nEncoders act as liason between JVM object and off-heap memory (the new formats introduced with Tungsten)"],"metadata":{}},{"cell_type":"markdown","source":["## Let's load a file\n\n1. select 'Tables'\n2. in new tab, select 'Create Table'\n3. we could really select anything (from file upload, s3, DBFS, or JDBC) here but for now we will upload the 'mallard.csv' from the vertica demo\n  (https://s3-us-west-2.amazonaws.com/cse599c-sp17/mallard.csv)\n4. select preview table\n5. we can name the table, select our file delimiter, etc.\n6. retrieve the DBFS path befor\n7. select 'create table'"],"metadata":{}},{"cell_type":"code","source":["# set file path\n# mallardFilePath = 'PATH.csv'"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["mallard = sqlContext.read.format(\"com.databricks.spark.csv\").options(header='true', inferschema='true', delimiter=',').load(mallardFilePath)\nmallard.count()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["sortMallard = mallard.sort(\"location-long\")"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["from pyspark.sql.functions import countDistinct, max\n\ncountDistinctMallard = mallard.select(\"location-long\", \"location-lat\")\\\n  .groupBy(\"location-long\", \"location-lat\")\\\n  .agg(countDistinct(\"location-long\").alias('c'))\\\n  .agg(max('c'))\n  \ncountDistinctMallard.head()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["# Use Case: On-Time Flight Performance\n\nThis notebook provides an analysis of On-Time Flight Performance and Departure Delays\n\nSource Data: \n* [OpenFlights: Airport, airline and route data](http://openflights.org/data.html)\n* [United States Department of Transportation: Bureau of Transportation Statistics (TranStats)](http://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&DB_Short_Name=On-Time)\n * Note, the data used here was extracted from the US DOT:BTS between 1/1/2014 and 3/31/2014*\n\nReferences:\n* [GraphFrames User Guide](http://graphframes.github.io/user-guide.html)\n* [GraphFrames: DataFrame-based Graphs (GitHub)](https://github.com/graphframes/graphframes)\n* [D3 Airports Example](http://mbostock.github.io/d3/talk/20111116/airports.html)"],"metadata":{}},{"cell_type":"markdown","source":["### Preparation\nExtract the Airports and Departure Delays information from S3 / DBFS"],"metadata":{}},{"cell_type":"code","source":["# Set File Paths\ntripdelaysFilePath = \"/databricks-datasets/flights/departuredelays.csv\"\nairportsnaFilePath = \"/databricks-datasets/flights/airport-codes-na.txt\""],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# Obtain airports dataset\nairportsna = sqlContext.read.format(\"com.databricks.spark.csv\").options(header='true', inferschema='true', delimiter='\\t').load(airportsnaFilePath)\nairportsna.registerTempTable(\"airports_na\")"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# Obtain departure Delays data\ndepartureDelays = sqlContext.read.format(\"com.databricks.spark.csv\").options(header='true').load(tripdelaysFilePath)\ndepartureDelays.registerTempTable(\"departureDelays\")\ndepartureDelays.cache()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["airportsna.printSchema()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["departureDelays.printSchema()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# offers programatic SQL type commands\nfrom pyspark.sql.functions import *\nsortDelays = departureDelays.sort(\"delay\")\nsortDelays.head(3)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# register the DataFrame as a temp table so that we can query it using SQL language\ndepartureDelays.registerTempTable(\"depature_delays\")\nsortDelays_sql = sqlContext.sql(\"SELECT * FROM depature_delays ORDER BY delay\")\nsortDelays_sql.head(3)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# We can also do more complex selections\nlongAvgDistByDest = departureDelays.groupBy(\"destination\")\\\n  .agg(avg(\"distance\").alias(\"avg_dist\"))\\\n  .where(\"avg_dist > 1000\")\nlongAvgDistByDest.head(3)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# and again with a declarative command\nlongAvgDistByDest_sql = sqlContext.sql(\"SELECT destination, avg(distance) AS avg_dist FROM depature_delays GROUP BY destination HAVING avg_dist > 1000\")\nlongAvgDistByDest_sql.head(3)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# we can also use the python fluent API to execute joins\nfrom pyspark.sql.functions import col\ndelayedSeaDest = departureDelays.join(airportsna, departureDelays[\"destination\"] == airportsna[\"IATA\"], 'inner')\\\n  .filter(col(\"origin\") == 'SEA')\\\n  .groupBy(\"destination\")\\\n  .agg(avg(\"delay\").alias(\"avg_delay\"))\\\n  .orderBy(col(\"avg_delay\").desc())\ndelayedSeaDest.head(3)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# and again with a declarative command\nairportsna.registerTempTable(\"airports\")\ndelayedSeaDest_sql = sqlContext.sql(\"SELECT destination, avg(delay) AS avg_delay FROM depature_delays, airports WHERE origin = 'SEA' AND destination = IATA GROUP BY destination ORDER BY -avg_delay\")\ndelayedSeaDest_sql.head(3)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["delayedSeaDest.explain()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["delayedSeaDest_sql.explain()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":22}],"metadata":{"name":"Tutorial","notebookId":3793756809609733},"nbformat":4,"nbformat_minor":0}
