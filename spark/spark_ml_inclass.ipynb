{"cells":[{"cell_type":"code","source":["##############################MLLib Example##############################\n# In this example, we are going to be building a classifer to try to predict if a flight in the US is going to be late or not\n# based on the flights' origin airport, destination airport, and distance. We will be using logistic regression to train our\n# model and then test it using a separate test set. At then end, we will report the percent error of the test set.\n\n# To get the data we will combine two datasets: one of the flight delays and one of information on where the airport is\n# located. The goal is to see how to combine datasets.\n\n# In the real world, you would likely have more attributes and more data, but this is just an example to show how to set up\n# and use the MLLib library.\n\n# First we need to collect out data\n# Paths to datasets\ntripdelaysFilePath = \"/databricks-datasets/flights/departuredelays.csv\"  # departure delay times of flights\nairportsnaFilePath = \"/databricks-datasets/flights/airport-codes-na.txt\"  # information about where airports are located\n\n# Obtain airports dataset\nairportsna = sqlContext.read.format(\"com.databricks.spark.csv\").options(header='true', inferschema='true', delimiter='\\t').load(airportsnaFilePath)\nairportsna.registerTempTable(\"airports_na\") # make table available in sql commands\n"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["# Obtain departure delay dataset\n\n\n"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# Now we need to do some transformations to our data\n# First, we get available IATA airport codes from the departure delay dataset\n# making sure to take all district codes from the origins and destinations\ntripIATA = sqlContext.sql(\"SELECT DISTINCT iata FROM (SELECT DISTINCT origin as iata FROM departureDelays UNION ALL SELECT DISTINCT destination as iata FROM departureDelays) a\")\ntripIATA.registerTempTable(\"tripIATA\")\n\n# Now we need to filter the airports so we only include airports\n# with at least one trip from the departureDelays dataset\n\n\n# This is a long query that selects the attributes we want from the data, joining airports with departuredelays\n# We also make departure delay a binary 0/1 label to use in our classification\ndepartureDelays_geo = sqlContext.sql(\"SELECT cast(f.date as int) as tripid, cast(f.delay as int) as delay, cast(f.delay < 0 as int) as delay_bool, cast(f.distance as int), f.origin as src, f.destination as dst, o.city as city_src, d.city as city_dst, o.state as state_src, d.state as state_dst from departuredelays f JOIN airports o ON o.iata = f.origin JOIN airports d ON d.iata = f.destination\") \n\n# RegisterTempTable\n\n\n# Cache and Count\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Now it's time for the machine learning in pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n# STRING INDEXER EXAMPLE: outputs categoryIndex after run in category\n#  id | category | categoryIndex\n# ----|----------|---------------\n#  0  | a        | 0.0\n#  1  | b        | 2.0\n#  2  | c        | 1.0\n#  3  | a        | 0.0\n#  4  | a        | 0.0\n#  5  | c        | 1.0\n\n# ONE HOT ENCODER: changes numeric value into binary vector with a 1 in the place of value\n\n# First, we need to build our ML pipeline\n# We begin by gathering the features we need in our feature vector (origin, destination, and distance)\n# We start by adding in functions to transform the categorical variables to numeric ones\n\n"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Use StringIndexer to turn label column into numeric column\n# We technically do not need to do this because we already have 0/1, but this is good\n# step for other classification labels (e.g. TRUE/FALSE, GOOD/BAD, ...)\nlabel_stringIdx = StringIndexer(inputCol = \"delay_bool\", outputCol = \"label\")\nstages += [label_stringIdx]\n\n# Now we need to add in our numeric columns\n\n# Assemble all columns into feature vector for classification\n\n\n\n# Run the feature transformations.\n#  fit() computes feature statistics as needed.\n#  transform() actually transforms the features.\n\n\n# Print the schema\ndataset.printSchema()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# Keep relevant columns\n\n# Split into training and test data\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n\n# Create initial LogisticRegression model\n\n# Train model with Training Data\n\n# Predict and test using Test Data\n"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# If you want to simply collect the results as a list of Rows\n#use collect to turn into list of Row elements\n#gets first row"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["from pyspark.sql import functions as F\n# Compute squared difference between label and prediciton\n\n# Sum the difference and turn into a list of one Row\n\n# Divide sum by the total number of rows\n"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":10}],"metadata":{"name":"MLLib Example In Class","notebookId":440737126099},"nbformat":4,"nbformat_minor":0}
