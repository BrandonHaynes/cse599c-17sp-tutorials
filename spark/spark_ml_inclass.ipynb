{"cells":[{"cell_type":"code","source":["tripdelaysFilePath = \"/databricks-datasets/flights/departuredelays.csv\"\nairportsnaFilePath = \"/databricks-datasets/flights/airport-codes-na.txt\"\n\n# Obtain airports dataset\nairportsna = sqlContext.read.format(\"com.databricks.spark.csv\").options(header='true', inferschema='true', delimiter='\\t').load(airportsnaFilePath)\nairportsna.registerTempTable(\"airports_na\") # make table available in sql commands\n"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["display(airportsna)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["airportsna"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["airports_na\n"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["departureDelays = sqlContext.read.format(\"com.databricks.spark.csv\").options(header='true').load(tripdelaysFilePath)\ndepartureDelays.registerTempTable(\"departureDelays\")\ndepartureDelays.cache() # caches table in memory (lazy); can also use sqlContext.cacheTable (greedy)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# Available IATA codes from the departuredelays sample dataset\ntripIATA = sqlContext.sql(\"select distinct iata from (select distinct origin as iata from departureDelays union all select distinct destination as iata from departureDelays) a\")\ntripIATA.registerTempTable(\"tripIATA\")\n\n# Only include airports with atleast one trip from the departureDelays dataset\nairports = sqlContext.sql(\"select f.IATA, f.City, f.State, f.Country from airports_na f join tripIATA t on t.IATA = f.IATA\")\nairports.registerTempTable(\"airports\")\nairports.cache()\n\ndepartureDelays_geo = sqlContext.sql(\"select cast(f.date as int) as tripid, cast(concat(concat(concat(concat(concat(concat('2014-', concat(concat(substr(cast(f.date as string), 1, 2), '-')), substr(cast(f.date as string), 3, 2)), ' '), substr(cast(f.date as string), 5, 2)), ':'), substr(cast(f.date as string), 7, 2)), ':00') as timestamp) as `localdate`, cast(f.delay as int) as delay, cast(f.delay < 0 as int) as delay_bool, cast(f.distance as int), f.origin as src, f.destination as dst, o.city as city_src, d.city as city_dst, o.state as state_src, d.state as state_dst from departuredelays f join airports o on o.iata = f.origin join airports d on d.iata = f.destination\") \n\n# RegisterTempTable\ndepartureDelays_geo.registerTempTable(\"departureDelays_geo\")\n\n# Cache and Count\ndepartureDelays_geo.cache()\ndepartureDelays_geo.count()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["display(sqlContext.sql(\"select count(distinct destination) from departuredelays\"))\n# display(departureDelays_geo)\n"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n\n# STRING INDEXER EXAMPLE: outputs categoryIndex after run in category\n#  id | category | categoryIndex\n# ----|----------|---------------\n#  0  | a        | 0.0\n#  1  | b        | 2.0\n#  2  | c        | 1.0\n#  3  | a        | 0.0\n#  4  | a        | 0.0\n#  5  | c        | 1.0\n\n# ONE HOT ENCODER: changes numeric value into binary vector with a 1 in the place of value\n\ncategoricalColumns = [\"src\", \"dst\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n  # Category Indexing with StringIndexer\n  stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n  # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n  encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"classVec\")\n  # Add stages.  These are not run here, but will run all at once later on.\n  stages += [stringIndexer, encoder]\n  \nlabel_stringIdx = StringIndexer(inputCol = \"delay_bool\", outputCol = \"label\")\nstages += [label_stringIdx]\n\nnumericCols = [\"distance\"]\nassemblerInputs = map(lambda c: c + \"classVec\", categoricalColumns) + numericCols\n\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nstages += [assembler]\n\npipeline = Pipeline(stages=stages)\n# Run the feature transformations.\n#  - fit() computes feature statistics as needed.\n#  - transform() actually transforms the features.\npipelineModel = pipeline.fit(departureDelays_geo)\ndataset = pipelineModel.transform(departureDelays_geo)\n\n# Keep relevant columns\nselectedcols = [\"label\", \"features\"] + departureDelays_geo.columns\n# dataset = dataset.select(selectedcols)\ndisplay(dataset)\n\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\nprint trainingData.count()\nprint testData.count()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n\n# Create initial LogisticRegression model\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n\n# Train model with Training Data\nlrModel = lr.fit(trainingData)\n\npredictions = lrModel.transform(testData)\n\npredictions.printSchema()\n\nselected = predictions.select(\"label\", \"prediction\", \"probability\")\ndisplay(selected)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nevaluator.evaluate(predictions)\n\nevaluator.getMetricName()\n\nprint lr.explainParams()"],"metadata":{},"outputs":[],"execution_count":10}],"metadata":{"name":"FirstNotebook","notebookId":1028108329909917},"nbformat":4,"nbformat_minor":0}
