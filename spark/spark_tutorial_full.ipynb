{"cells":[{"cell_type":"markdown","source":["# RDDs, Dataframes, and Datasets\n\n## RDDs\n\nResilient Distributed Datasets (We talked about these!). A new range of API's has been introduced to let people take advantage of Spark's parallel execution framework and fault tolerance without making the same set of mistakes.\n\n## Dataframes\n\n- RDD's with named *untyped* columns.\n- Columnar storage\n  - Similar optimizations for OLAP queries as vertica\n- Memory Management (Tungsten)\n  - direct control of data storage in memory\n    - cpu cache, and read ahead\n  - largest source of performance increase\n- avoids java serialization (or other not as slow but still slow serialization)\n  - Kryo serialization\n  - compression\n- no garbage collection overhead\n- Execution plans (Catalyst Optimizer)\n  - rule based instead of cost-based optimizer\n\n## Datasets\n\nadds to Dataframes\n- compile time safety\n- API only available through the scala (python has no type safety)\n\nEncoders act as liason between JVM object and off-heap memory (the new formats introduced with Tungsten)"],"metadata":{}},{"cell_type":"markdown","source":["## Let's load a file\n\n1. select 'Tables'\n2. in new tab, select 'Create Table'\n3. we could really select anything (from file upload, s3, DBFS, or JDBC) here but for now we will upload the 'mallard.csv' from the vertica demo\n  (https://s3-us-west-2.amazonaws.com/cse599c-sp17/mallard.csv)\n4. select preview table\n5. we can name the table, select our file delimiter, etc.\n6. retrieve the DBFS path befor\n7. select 'create table'"],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# set file path for mallard.csv import\n#mallardFilePath = '/FileStore/tables/<uuid>/mallard.csv'\nmallardFilePath = '/FileStore/tables/7n7zqjcx1492482730270/mallard.csv'\n#mallardFilePath = ''\n\n# mallard.csv header\n# event-id,timestamp,location-long,location-lat"],"metadata":{"collapsed":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Import raw rdd convert to dataframe\nfrom pyspark.sql import types\n\n# Create RDD from csv file\n# pysqark RDD documentation: http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD\nmallard_rdd = sc.textFile(mallardFilePath)\n\nmallard_rdd.take(10)\nmallard_rdd.count()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["def rdd_head(rdd,n=3):\n  displayHTML(\"Count: {} <br> {}\".format(rdd.count(), \"<br>\".join(str(row) for row in rdd.take(n))) )\n\nrdd_head(mallard_rdd)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["#Skip header row\nheader_row = mallard_rdd.first()\nmallard_rdd = mallard_rdd.filter(lambda row : not row.startswith('event-id'))\nrdd_head(mallard_rdd)\n"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["#Split rows and convert\nfrom datetime import datetime\n\ndef make_row(row):\n  row = row.split(',')\n  return ( int(row[0]), datetime.strptime(row[1],'%Y-%m-%d %H:%M:%S.000') , float(row[2]) , float(row[3]) )\n\nmallard_rdd = mallard_rdd.map(make_row)\n\nrdd_head(mallard_rdd)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["#Question: What day had the most sightings?"],"metadata":{}},{"cell_type":"code","source":["# Use groupBy and map\nmallard_rdd_days = mallard_rdd.groupBy( lambda row: str(row[1].date()) ).map(lambda row: (row[0],len(row[1])) )\n\nrdd_head(mallard_rdd_days)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# Sort by count descending\nmallard_rdd_days_descending = mallard_rdd_days.sortBy( lambda row: row[1], ascending=False)\nrdd_head(mallard_rdd_days_descending)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# pyspark Dataframe documentation: http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame\nmallard_dataframe = mallard_rdd.toDF()\nmallard_dataframe.printSchema()\nmallard_dataframe.count()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Make schema for mallard dataframe\nfrom datetime import datetime\n\nheader_row = header_row.split(',')\ndef row_func(row):\n  return { header : obj for header , obj in zip(header_row,row) }\n    \n\nmallard_dataframe = mallard_rdd.map(row_func).toDF()\nmallard_dataframe.printSchema()\nmallard_dataframe.head(10)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["mallard = sqlContext.read.format(\"com.databricks.spark.csv\").options(header='true', inferschema='true', delimiter=',').load(mallardFilePath)\ndisplay(mallard)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":["sortMallard = mallard.sort(\"location-long\",\"location-lat\")\ndisplay(sortMallard)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["#Question: What day had the most sightings?"],"metadata":{}},{"cell_type":"code","source":["#pyspark sql functions: http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#module-pyspark.sql.functions\n# offers programatic SQL type commands\nimport pyspark.sql.functions as sf\n\nmallard_days = mallard \\\n  .select(sf.col('timestamp').cast('date').alias('date')) \\\n  .groupBy('date') \\\n  .agg(sf.count(\"date\").alias(\"count\")) \\\n  .sort(sf.col(\"count\").desc())\n\ndisplay(mallard_days)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# A more complicated example to show the expressiveness of pysqark \n# Calculate number of sightings in 1.1km squares (~= 1 decimal places of latitude and longitude )\n\ncountOneKM = mallard.withColumn(\"round-long\",sf.round(\"location-long\",1)) \\\n  .withColumn(\"round-lat\",sf.round(\"location-lat\",1))\\\n  .select(\"round-long\",\"round-lat\") \\\n  .groupBy(\"round-long\", \"round-lat\") \\\n  .agg(sf.count(\"round-long\").alias('location-bin')) \\\n  .sort(sf.col(\"location-bin\").desc())\ndisplay(countOneKM)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["# Use Case: On-Time Flight Performance\n\nThis notebook provides an analysis of On-Time Flight Performance and Departure Delays\n\nSource Data: \n* [OpenFlights: Airport, airline and route data](http://openflights.org/data.html)\n* [United States Department of Transportation: Bureau of Transportation Statistics (TranStats)](http://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&DB_Short_Name=On-Time)\n * Note, the data used here was extracted from the US DOT:BTS between 1/1/2014 and 3/31/2014*\n\nReferences:\n* [GraphFrames User Guide](http://graphframes.github.io/user-guide.html)\n* [GraphFrames: DataFrame-based Graphs (GitHub)](https://github.com/graphframes/graphframes)\n* [D3 Airports Example](http://mbostock.github.io/d3/talk/20111116/airports.html)"],"metadata":{}},{"cell_type":"markdown","source":["### Preparation\nExtract the Airports and Departure Delays information from S3 / DBFS"],"metadata":{}},{"cell_type":"code","source":["# Set File Paths\ntripdelaysFilePath = \"/databricks-datasets/flights/departuredelays.csv\"\nairportsnaFilePath = \"/databricks-datasets/flights/airport-codes-na.txt\""],"metadata":{"collapsed":true},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# Obtain airports dataset\nairportsna = sqlContext.read.format(\"com.databricks.spark.csv\").options(header='true', inferschema='true', delimiter='\\t').load(airportsnaFilePath)\nairportsna.registerTempTable(\"airports_na\")"],"metadata":{"collapsed":true},"outputs":[],"execution_count":22},{"cell_type":"code","source":["# Obtain departure Delays data\ndepartureDelays = sqlContext.read.format(\"com.databricks.spark.csv\").options(header='true').load(tripdelaysFilePath)\ndepartureDelays.registerTempTable(\"departureDelays\")\ndepartureDelays.cache()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":23},{"cell_type":"code","source":["airportsna.printSchema()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":24},{"cell_type":"code","source":["departureDelays.printSchema()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":25},{"cell_type":"code","source":["# use pysqark.sql.functions which is already imported as sf\nsortDelays = departureDelays.sort(\"delay\")\nsortDelays.head(3)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":26},{"cell_type":"code","source":["# register the DataFrame as a temp table so that we can query it using SQL language\ndepartureDelays.registerTempTable(\"depature_delays\")\nsortDelays_sql = sqlContext.sql(\"SELECT * FROM depature_delays ORDER BY delay\")\nsortDelays_sql.head(3)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":27},{"cell_type":"code","source":["# We can also do more complex selections\nlongAvgDistByDest = departureDelays.groupBy(\"destination\")\\\n  .agg(sf.avg(\"distance\").alias(\"avg_dist\"))\\\n  .where(\"avg_dist > 1000\")\nlongAvgDistByDest.head(3)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":28},{"cell_type":"code","source":["# and again with a declarative command\nlongAvgDistByDest_sql = sqlContext.sql(\"SELECT destination, avg(distance) AS avg_dist FROM depature_delays GROUP BY destination HAVING avg_dist > 1000\")\nlongAvgDistByDest_sql.head(3)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":29},{"cell_type":"code","source":["# we can also use the python fluent API to execute joins\nfrom pyspark.sql.functions import col\ndelayedSeaDest = departureDelays.join(airportsna, departureDelays[\"destination\"] == airportsna[\"IATA\"], 'inner')\\\n  .filter(col(\"origin\") == 'SEA')\\\n  .groupBy(\"destination\")\\\n  .agg(sf.avg(\"delay\").alias(\"avg_delay\"))\\\n  .orderBy(sf.col(\"avg_delay\").desc())\ndelayedSeaDest.head(3)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":30},{"cell_type":"code","source":["# and again with a declarative command\nairportsna.registerTempTable(\"airports\")\ndelayedSeaDest_sql = sqlContext.sql(\"SELECT destination, avg(delay) AS avg_delay FROM depature_delays, airports WHERE origin = 'SEA' AND destination = IATA GROUP BY destination ORDER BY -avg_delay\")\ndelayedSeaDest_sql.head(3)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":31},{"cell_type":"code","source":["delayedSeaDest.explain()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":32},{"cell_type":"code","source":["delayedSeaDest_sql.explain()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":33},{"cell_type":"code","source":[""],"metadata":{"collapsed":true},"outputs":[],"execution_count":34}],"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython2","codemirror_mode":{"name":"ipython","version":2},"version":"2.7.13","nbconvert_exporter":"python","file_extension":".py"},"name":"Tutorial","notebookId":3391814867969884},"nbformat":4,"nbformat_minor":0}
