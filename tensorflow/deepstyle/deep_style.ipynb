{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Neural Art Style Transfer\n",
    "=============\n",
    "\n",
    "(This demo is adapted from [neuralart_tensorflow](https://github.com/ckmarkoh/neuralart_tensorflow) and [this paper review](https://github.com/tensorflow/magenta/blob/master/magenta/reviews/styletransfer.md))\n",
    "\n",
    "Introduction\n",
    "----------\n",
    "In late August 2015, Gatys et al. from The University of Tübingen published [A Neural Algorithm of Artistic Style](http://arxiv.org/pdf/1508.06576v2.pdf). It demonstrated a way to present one piece of artwork in the style of a separate piece.  The stunning results gained a lot of public attention. https://deepdreamgenerator.com/ Provides a platform for creating Deep Dream & Deep Style arts without writing code.\n",
    "\n",
    "The style transfer model is built on top of a deep conv net: [**VGG19**](http://www.robots.ox.ac.uk/~vgg/research/very_deep/), which is used in the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC). VGG19 is trained on more than a million images and can classify images into 1000 object categories. One key insight is that such networks trained to perform object detection end up learning to separate content from style.\n",
    "\n",
    "At a high level, the general technique is pretty straightforward. First, define a function that describes the quality of a generated image intended to match a particular content and style. Then, synthesize an image that maximizes that score. This is done by starting with a noisy content image and then optimizing using backpropagation, just like what’s done for training neural nets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements\n",
    "---------\n",
    "(Only if you want to follow along)\n",
    "\n",
    "This demo requires a copy of the [pretrained VGG19 model data](https://drive.google.com/file/d/0B8QJdgMvQDrVU2cyZjFKU1RrLUU/view?usp=sharing) placed in the same directory.\n",
    "\n",
    "GPU is recommended for this task. With a GPU it should take several miniutes to get meaningful results, wheares with CPU only it may take several hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow Implementation\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import scipy.misc\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image related utils. A image is represented as a tensor of shape `(1, IMAGE_H, IMAGE_W, 3)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "IMAGE_W = 400  # Small fixed image size for demo\n",
    "IMAGE_H = 300 \n",
    "OUTOUT_DIR = './results'\n",
    "\n",
    "MEAN_VALUES = np.array([123, 117, 104]).reshape((1,1,1,3))  # RGB means\n",
    "\n",
    "def read_image(path):\n",
    "    image = scipy.misc.imread(path)\n",
    "    image = image[np.newaxis,:IMAGE_H,:IMAGE_W,:] \n",
    "    image = image - MEAN_VALUES\n",
    "    return image\n",
    "\n",
    "def write_image(path, image):\n",
    "    image = image + MEAN_VALUES\n",
    "    image = image[0]\n",
    "    image = np.clip(image, 0, 255).astype('uint8')\n",
    "    scipy.misc.imsave(path, image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Functions for rebuilding the vgg network from a file with pretrained parameters. The parameters of the conv net layers will not change during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_net(ntype, nin, nwb=None):\n",
    "    if ntype == 'conv':\n",
    "        return tf.nn.relu(tf.nn.conv2d(nin, nwb[0], strides=[1, 1, 1, 1],\n",
    "                                       padding='SAME') + nwb[1])\n",
    "    elif ntype == 'pool':\n",
    "        return tf.nn.avg_pool(nin, ksize=[1, 2, 2, 1],\n",
    "                                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def get_weight_bias(vgg_layers, i,):\n",
    "    weights = vgg_layers[i][0][0][0][0][0]\n",
    "    weights = tf.constant(weights)\n",
    "    bias = vgg_layers[i][0][0][0][0][1]\n",
    "    bias = tf.constant(np.reshape(bias, (bias.size)))\n",
    "    return weights, bias\n",
    "\n",
    "def build_vgg19(path):\n",
    "    net = {}\n",
    "    vgg_rawnet = scipy.io.loadmat(path)\n",
    "    vgg_layers = vgg_rawnet['layers'][0]\n",
    "    net['input'] = tf.Variable(np.zeros((1, IMAGE_H, IMAGE_W, 3)).astype('float32'))\n",
    "    net['conv1_1'] = build_net('conv',net['input'],get_weight_bias(vgg_layers,0))\n",
    "    net['conv1_2'] = build_net('conv',net['conv1_1'],get_weight_bias(vgg_layers,2))\n",
    "    net['pool1']   = build_net('pool',net['conv1_2'])\n",
    "    net['conv2_1'] = build_net('conv',net['pool1'],get_weight_bias(vgg_layers,5))\n",
    "    net['conv2_2'] = build_net('conv',net['conv2_1'],get_weight_bias(vgg_layers,7))\n",
    "    net['pool2']   = build_net('pool',net['conv2_2'])\n",
    "    net['conv3_1'] = build_net('conv',net['pool2'],get_weight_bias(vgg_layers,10))\n",
    "    net['conv3_2'] = build_net('conv',net['conv3_1'],get_weight_bias(vgg_layers,12))\n",
    "    net['conv3_3'] = build_net('conv',net['conv3_2'],get_weight_bias(vgg_layers,14))\n",
    "    net['conv3_4'] = build_net('conv',net['conv3_3'],get_weight_bias(vgg_layers,16))\n",
    "    net['pool3']   = build_net('pool',net['conv3_4'])\n",
    "    net['conv4_1'] = build_net('conv',net['pool3'],get_weight_bias(vgg_layers,19))\n",
    "    net['conv4_2'] = build_net('conv',net['conv4_1'],get_weight_bias(vgg_layers,21))\n",
    "    net['conv4_3'] = build_net('conv',net['conv4_2'],get_weight_bias(vgg_layers,23))\n",
    "    net['conv4_4'] = build_net('conv',net['conv4_3'],get_weight_bias(vgg_layers,25))\n",
    "    net['pool4']   = build_net('pool',net['conv4_4'])\n",
    "    net['conv5_1'] = build_net('conv',net['pool4'],get_weight_bias(vgg_layers,28))\n",
    "    net['conv5_2'] = build_net('conv',net['conv5_1'],get_weight_bias(vgg_layers,30))\n",
    "    net['conv5_3'] = build_net('conv',net['conv5_2'],get_weight_bias(vgg_layers,32))\n",
    "    net['conv5_4'] = build_net('conv',net['conv5_3'],get_weight_bias(vgg_layers,34))\n",
    "    net['pool5']   = build_net('pool',net['conv5_4'])\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the content loss and style loss.\n",
    "- Content loss is simply the *L2 difference at a particular layer*. More specifically, for image X and content image C, compute the L2 loss between the output of X and that of C, at the layer conv4_2 in the conv net.\n",
    "- For style loss, the paper uses [Gram matrices](https://en.wikipedia.org/wiki/Gramian_matrix). Empirically, these are a good proxy for feature correlations, and so the *L2 difference between the Gram matrix* of one image and that of another works well as a way to compare how close they are in style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_content_loss(p, x):\n",
    "    M = p.shape[1] * p.shape[2]\n",
    "    N = p.shape[3]\n",
    "    loss = (1. / (2 * N**0.5 * M**0.5)) * tf.reduce_sum(tf.pow((x - p), 2))\n",
    "    return loss\n",
    "\n",
    "def gram_matrix(x, area, depth):\n",
    "    x1 = tf.reshape(x,(area,depth))\n",
    "    g = tf.matmul(tf.transpose(x1), x1)\n",
    "    return g\n",
    "\n",
    "def gram_matrix_val(x, area, depth):\n",
    "    x1 = x.reshape(area,depth)\n",
    "    g = np.dot(x1.T, x1)\n",
    "    return g\n",
    "\n",
    "def build_style_loss(p, x):\n",
    "    M = p.shape[1] * p.shape[2]\n",
    "    N = p.shape[3]\n",
    "    A = gram_matrix_val(p, M, N )\n",
    "    G = gram_matrix(x, M, N )\n",
    "    loss = (1. / (4 * N**2 * M**2)) * tf.reduce_sum(tf.pow((G - A), 2))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "VGG_MODEL = 'imagenet-vgg-verydeep-19.mat'  # pretrained deep conv net model\n",
    "INI_NOISE_RATIO = 0.7\n",
    "STYLE_STRENGTH = 300  # How much to favor style over the original content\n",
    "ITERATION = 2000\n",
    "\n",
    "# Layers in the VGG19 conv net for calculating the losses.\n",
    "CONTENT_LAYERS =[('conv4_2',1.)]\n",
    "STYLE_LAYERS=[('conv1_1',1.),('conv2_1',1.),('conv3_1',1.),('conv4_1',1.),('conv5_1',1.)]\n",
    "\n",
    "def stylize(content, style):\n",
    "    noise_img = np.random.uniform(-20, 20, (1, IMAGE_H, IMAGE_W, 3)).astype('float32')\n",
    "    content_img = read_image(content)\n",
    "    style_img = read_image(style)\n",
    "    \n",
    "    net = build_vgg19(VGG_MODEL)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        sess.run([net['input'].assign(content_img)])\n",
    "        \n",
    "        # build the content loss\n",
    "        cost_content = sum(map(lambda l,: l[1] * build_content_loss(sess.run(net[l[0]]) ,  net[l[0]])\n",
    "            , CONTENT_LAYERS))\n",
    "        \n",
    "        # build the style loss\n",
    "        sess.run([net['input'].assign(style_img)])\n",
    "        cost_style = sum(map(lambda l: l[1] * build_style_loss(sess.run(net[l[0]]) ,  net[l[0]])\n",
    "            , STYLE_LAYERS))\n",
    "\n",
    "        # Minimizing a combination of content loss and style loss\n",
    "        cost_total = cost_content + STYLE_STRENGTH * cost_style\n",
    "        optimizer = tf.train.AdamOptimizer(2.0)\n",
    "        train = optimizer.minimize(cost_total)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # The initial image is a noisy version of the content\n",
    "        sess.run(net['input'].assign( INI_NOISE_RATIO * noise_img + (1.-INI_NOISE_RATIO) * content_img ))\n",
    "\n",
    "        if not os.path.exists(OUTOUT_DIR):\n",
    "                os.mkdir(OUTOUT_DIR)\n",
    "        print(\"Training started...\")\n",
    "        \n",
    "        for i in range(ITERATION + 1):\n",
    "            sess.run(train)\n",
    "            if i % 100 == 0:\n",
    "                result_img = sess.run(net['input'])\n",
    "                print(str(i) + \"\\t\" + str(sess.run(cost_total)))\n",
    "                write_image(os.path.join(OUTOUT_DIR,'%s.png'%(str(i).zfill(4))),result_img)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to try the algoritm on input images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stylize(content=\"images/Quad.jpg\", style=\"images/StarryNight.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Style ![Original](images/StarryNight.jpg)\n",
    "Original ![Original](images/Quad.jpg)\n",
    "0 ![0](results/0000.png)\n",
    "100 ![100](results/0100.png)\n",
    "500 ![100](results/0500.png)\n",
    "1000 ![1000](results/1000.png)\n",
    "1500 ![1000](results/1500.png)\n",
    "2000 ![1000](results/2000.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
